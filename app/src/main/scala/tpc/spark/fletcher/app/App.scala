/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package tpc.spark.fletcher.app

import tpc.spark.fletcher.app.utils.ScoptConfig.Config
import tpc.spark.fletcher.app.utils.InitSpark
import tpc.spark.fletcher.app.utils.Dataset
import tpc.spark.fletcher.app.utils.TPCQueries
import scopt.OParser
import java.io.File
import scala.reflect.io.Directory
import scala.collection.mutable.ListBuffer

import org.apache.spark.sql.{SparkSession, SparkSessionExtensions, Column}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.types.DataType
object App extends InitSpark {

  var spark: SparkSession = _

  def main(args: Array[String]): Unit = {

    val builder = OParser.builder[Config]

    val parser1 = {
      import builder._
      OParser.sequence(
        programName("./gradlew run --args="),
        head(
          "TPC Spark App (Hardware enabled): Developed by ABS TU Delft",
          "0.1"
        ),
        // option -f, --foo
        help('h', "help").text("prints this usage text"),
        opt[Unit]('v', "verbose")
          .action((_, c) => c.copy(verbose = true))
          .text("verbose is a flag"),
        opt[String]('e', "extension")
          .valueName("[vanilla|fletcher]")
          .action((x, c) => c.copy(extension = x))
          .validate(x =>
            if (x == "vanilla" || x == "fletcher") success
            else failure("Only modes of fletcher or vanilla is available"))
          .text(
            "Run application with either Vanilla Spark or Fletcher(Hardware enabled) Spark. Default is vanilla."
          ),
        note(sys.props("line.separator")),
        cmd("benchmark")
          .action((_, c) => c.copy(mode = "benchmark"))
          .text("benchmark command runs any selected query")
          .children(
            opt[String]("type")
              .valueName("[H|DS]")
              .abbr("t")
              .action((x, c) => c.copy(benchmark_type = x))
              .text("H or DS"),
            opt[String]("file")
              .valueName("file_path")
              .abbr("f")
              .action((x, c) => c.copy(file_path = x))
              .text("file_path"),
            opt[Boolean]("optimize")
              .abbr("opt")
              .action((_, c) => c.copy(optimize = true))
              .text("Optimize java jni buffers"),
            opt[Seq[Int]]("queries")
              .valueName("1,2...")
              .abbr("q")
              .action((x, c) => c.copy(queries = x))
              .validate(x => {
                var res: Boolean = true
                var no: List[Int] = List()
                x.foreach { el =>
                  if (el < 0 || el > 1000) {
                    res = false
                    no ::= el
                  }
                }
                if (res == true) success
                else failure("Query numbers provided do not exist: " + no)
              })
              .text("Queries to benchmark")
          ),
        note(sys.props("line.separator")),
        cmd("generate-parquet")
          .action((_, c) => c.copy(mode = "generate_parquet"))
          .text(
            "Generate parquet files.User has to provide DATASET_DIR environment variable."
          )
          .children(
            opt[String]("dataset")
              .valueName("[lineitem|part]")
              .action((x, c) => c.copy(dataset = x))
              .text("lineitem, part .."),
            opt[Int]("num_rows")
              .valueName("1")
              .action((x, c) => c.copy(num_rows = x))
              .text("Default is 100"),
            opt[Int]("num_partitions")
              .valueName("1")
              .action((x, c) => c.copy(num_partitions = x))
              .text("Default is 1")
          )
      )
    }
    // OParser.parse returns Option[Config]
    OParser.parse(parser1, args, Config()) match {
      case Some(config) =>
        println("RUNNING APP TYPE: " + config.extension)
        spark = init(config.extension)
        //spark.sqlContext.sql("SET spark.sql.parquet.filterPushdown=false")
        config.mode match {
          case "benchmark" =>
            for (t <- config.queries) {
              clearCache(spark)
              println("Running query No: " + t)
              val lineitem_schema = StructType(
                Seq(
                  StructField("l_orderkey", IntegerType, false),
                  StructField("l_partkey", IntegerType, false),
                  StructField("l_suppkey", IntegerType, false),
                  StructField("l_linenumber", IntegerType, false),
                  StructField("l_quantity", DoubleType, false),
                  StructField("l_extendedprice", DoubleType, false),
                  StructField("l_discount", DoubleType, false),
                  StructField("l_tax", DoubleType, false),
                  StructField("l_returnflag", StringType, false),
                  StructField("l_linestatus", StringType, false),
                  StructField("l_shipdate", DateType, false),
                  StructField("l_commitdate", DateType, false),
                  StructField("l_receiptdate", DateType, false),
                  StructField("l_shipinstruct", StringType, false),
                  StructField("l_shipmode", StringType, false),
                  StructField("l_comment", StringType, false)
                ))
              val lineitemdf = spark.read
                .schema(lineitem_schema)
                .parquet(config.file_path)
                .createOrReplaceTempView("lineitem")
              //val partdf = spark.read.parquet("/home/yyunon/thesis_journals/resources/tpc-spark-fletcher/data/part/part.parquet").createOrReplaceTempView("part")
              val query = "query_" + t + ".sql"
              val query_string =
                new TPCQueries(spark, config.benchmark_type, query).load_query()
              val result = spark.sqlContext.sql(query_string)
              if (config.verbose == true)
                result.explain(mode = "extended")
              if (config.extension == "fletcher")
                //spark.time(result.select(col("revenue").reduce(_+_)).show())
                spark.time(result.agg(sum("revenue")).show())
              else
                spark.time(result.show())
            }
          case "generate_parquet" =>
            println("Generating parquet file")
            spark.conf.set(
              "spark.sql.parquet.compression.codec",
              "uncompressed"
            )
            // Delete old run
            val file_path = "../data/" + config.dataset
            val directory = new Directory(new File(file_path))
            directory.deleteRecursively()

            val df =
              new Dataset(spark, config.dataset)
                .load_dataset(config.num_partitions, config.num_rows)
            val df_t = df.select(col("l_extendedprice"),
                                 col("l_discount"),
                                 col("l_quantity"),
                                 col("l_shipdate"))
            df_t.show()
            df_t.write.parquet(s"../data/" + config.dataset)
          case _ =>
            println("Unknown mode.")
            sys.exit(1)
        }
        var dur_list: ListBuffer[Seq[Long]] = ListBuffer()
        var file_list: ListBuffer[Seq[Long]] = ListBuffer()
        val counter: Int = 4
        var i: Int = 0
        var acc_d: Long = 0
        var acc_f: Long = 0
        metrics.remove(0)
        println(metrics)
        for (x <- metrics) {
          if ((i > 0) && ((i % counter - 1) == 0)) {
            dur_list += Seq[Long](acc_d / 4)
            file_list += Seq[Long](acc_f / 4)
            acc_d = 0
            acc_f = 0
          }
          acc_f += x(0)
          acc_d += x(2)
          i += 1
        }
        println(dur_list)
        println(file_list)
        spark.close
      case _ =>
    }
  }
}
